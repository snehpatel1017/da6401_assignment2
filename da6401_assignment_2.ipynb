{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, SubsetRandomSampler, Subset\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as patches\nimport matplotlib.patheffects as path_effects\nimport matplotlib.colors as mcolors\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport wandb\nimport gc\nimport os\nimport io\nimport PIL\nimport PIL.Image\nimport copy\nimport random\nimport math\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ngc.collect()\ntorch.cuda.empty_cache()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nCLASSES = [\n    'Amphibia',\n    'Animalia',\n    'Arachnida',\n    'Aves',\n    'Fungi',\n    'Insecta',\n    'Mammalia',\n    'Mollusca',\n    'Plantae',\n    'Reptilia'\n    ]\n\nif device.type == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory Usage:\")\n    print(f\"Allocated: {round(torch.cuda.memory_allocated(0)/1024**3, 1)} GB\")\n    print(f\"Cached: {round(torch.cuda.memory_reserved(0)/1024**3, 1)} GB\")\n\n\n# Custom transform to handle image orientation\nclass RotateIfNeeded:\n    def __call__(self, img):\n        width, height = img.size\n        if height > width:\n            return img.transpose(2)  # Rotate 90 degrees if portrait\n        return img\n\n\ndef create_prediction_grid(model, test_loader, class_names):\n    model.eval()\n    all_images = []\n    all_preds = []\n    all_targets = []\n    all_probs = []\n    \n    # Collect images and predictions\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            if len(all_images) >= 30:  # We need 30 images for 10x3 grid\n                break\n                \n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            probs = torch.nn.functional.softmax(outputs, dim=1)\n            confidence, preds = torch.max(probs, 1)\n            \n            # Convert to CPU and save\n            batch_images = inputs.cpu().numpy()\n            batch_preds = preds.cpu().numpy()\n            batch_confidence = confidence.cpu().numpy()\n            \n            # Add samples to our collections\n            for i in range(min(len(batch_images), 30 - len(all_images))):\n                all_images.append(batch_images[i])\n                all_preds.append(batch_preds[i])\n                all_targets.append(targets[i].item())\n                all_probs.append(batch_confidence[i])\n    \n    # Set up the figure with a custom layout\n    fig = plt.figure(figsize=(18, 30))\n    gs = gridspec.GridSpec(10, 3, figure=fig, wspace=0.2, hspace=0.4)\n    \n    # Color mapping for correct/incorrect predictions\n    correct_color = '#2ecc71'  # Green\n    incorrect_color = '#e74c3c'  # Red\n    \n    # Create custom title for the grid\n    fig.suptitle('Model Predictions on Test Data', fontsize=24, y=0.92)\n    \n    # For each image in our grid\n    for i in range(10):\n        for j in range(3):\n            idx = i * 3 + j\n            if idx < len(all_images):\n                # Get image and prediction info\n                img = all_images[idx].transpose(1, 2, 0)\n                # Denormalize the image\n                img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n                img = np.clip(img, 0, 1)\n                \n                pred_class = all_preds[idx]\n                true_class = all_targets[idx]\n                confidence = all_probs[idx]\n                is_correct = pred_class == true_class\n                color = correct_color if is_correct else incorrect_color\n                \n                # Create subplot\n                ax = fig.add_subplot(gs[i, j])\n                \n                # Display image with custom border\n                ax.imshow(img)\n                ax.set_xticks([])\n                ax.set_yticks([])\n                \n                # Add a colored border based on correctness\n                for spine in ax.spines.values():\n                    spine.set_linewidth(6)\n                    spine.set_color(color)\n                \n                # Create a top banner with the class name\n                pred_name = class_names[pred_class]\n                true_name = class_names[true_class]\n                \n                # Add prediction text\n                ax.set_title(f\"Prediction: {pred_name}\", \n                           fontsize=12, color='white', \n                           bbox=dict(facecolor=color, alpha=0.9, pad=5))\n                \n                # Add ground truth text below image\n                ax.annotate(f\"Ground Truth: {true_name}\", \n                          xy=(0.5, -0.03), xycoords='axes fraction', \n                          fontsize=10, ha='center', va='top',\n                          bbox=dict(facecolor='gray', alpha=0.8, pad=3))\n                \n                # Add confidence score\n                ax.annotate(f\"Confidence: {confidence:.1%}\", \n                          xy=(0.5, 1.02), xycoords='axes fraction', \n                          fontsize=10, ha='center',\n                          bbox=dict(facecolor='#3498db', alpha=0.8, pad=1))\n                \n                # Add an icon to indicate correctness\n                if is_correct:\n                    ax.annotate('✓', xy=(0.95, 0.95), xycoords='axes fraction', \n                              fontsize=18, ha='right', va='top', color='white',\n                              bbox=dict(facecolor=color, alpha=0.8, boxstyle='circle'))\n                else:\n                    ax.annotate('✗', xy=(0.95, 0.95), xycoords='axes fraction', \n                              fontsize=18, ha='right', va='top', color='white',\n                              bbox=dict(facecolor=color, alpha=0.8, boxstyle='circle'))\n                \n                # Add a small confidence bar\n                bar_width = confidence\n                bar_height = 0.04\n                bar_y = 0.01\n                ax.add_patch(patches.Rectangle(\n                    (0.1, bar_y), 0.8 * bar_width, bar_height,\n                    transform=ax.transAxes, facecolor='#f39c12', alpha=0.8\n                ))\n                # Add background for full confidence bar\n                ax.add_patch(patches.Rectangle(\n                    (0.1, bar_y), 0.8, bar_height,\n                    transform=ax.transAxes, facecolor='#bdc3c7', alpha=0.3\n                ))\n    \n    # Add a legend\n    legend_elements = [\n        patches.Patch(facecolor=correct_color, label='Correct Prediction'),\n        patches.Patch(facecolor=incorrect_color, label='Incorrect Prediction')\n    ]\n    fig.legend(handles=legend_elements, loc='upper center', ncol=2, fontsize=12)\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for title and legend\n    \n    # Save the figure\n    plt.savefig('test_predictions.png', dpi=300, bbox_inches='tight')\n    \n    # Log to wandb if desired\n    try:\n       \n        wandb.log({\"test_predictions_grid\": wandb.Image(fig)})\n    except:\n        print(\"Couldn't log to wandb, continuing...\")\n    \n    return fig\n\n# Function to create data loaders with stratified sampling\ndef create_data_loaders(train_dir, test_dir, input_size=(224, 224), batch_size=32,\n                        data_augmentation=False, val_ratio=0.2, num_workers=2):\n    \"\"\"\n    Create data loaders for training, validation, and testing.\n    Uses stratified sampling to ensure class balance in validation set.\n    \"\"\"\n    # Base transform\n    base_transform = [\n        RotateIfNeeded(),\n        transforms.Resize(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]\n\n    # Training transform with optional augmentation\n    if data_augmentation:\n        train_transform = transforms.Compose([\n            base_transform[0],  # RotateIfNeeded\n            base_transform[1],  # Resize\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            base_transform[2],  # ToTensor\n            base_transform[3]   # Normalize\n        ])\n    else:\n        train_transform = transforms.Compose(base_transform)\n\n    # Test transform (no augmentation)\n    test_transform = transforms.Compose(base_transform)\n\n    # Load datasets\n    train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n    test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n\n    # Create stratified train/validation split\n    targets = np.array([sample[1] for sample in train_dataset.samples])\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=42)\n    train_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n\n    # Create samplers\n    train_sampler = SubsetRandomSampler(train_idx)\n    val_sampler = SubsetRandomSampler(val_idx)\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, sampler=train_sampler,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        train_dataset, batch_size=batch_size, sampler=val_sampler,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n\n    print(f\"Training set: {len(train_idx)} images\")\n    print(f\"Validation set: {len(val_idx)} images\")\n    print(f\"Test set: {len(test_dataset)} images\")\n    print(f\"Number of classes: {len(train_dataset.classes)}\")\n\n    return train_loader, val_loader , test_loader, len(train_dataset.classes)\n\n# Enhanced FlexibleCNN model with support for different filter organizations\nclass FlexibleCNN(nn.Module):\n    def __init__(self, in_channels=3, num_filters=32, filter_size=3,\n                 activation_fn=nn.ReLU, filter_organization='same',\n                 use_batchnorm=False, dropout_rate=0.0,\n                 dense_neurons=128, num_classes=10, input_size=(224, 224)):\n        super(FlexibleCNN, self).__init__()\n\n        # Determine filter counts based on organization strategy\n        if filter_organization == 'same':\n            filters = [num_filters] * 5\n        elif filter_organization == 'double':\n            filters = [num_filters * (2**i) for i in range(5)]\n        elif filter_organization == 'half':\n            filters = [num_filters // (2**(i)) for i in range(5)]\n            filters = [max(16, f) for f in filters]  # Ensure minimum filter count\n\n        # Build layers list\n        layers = []\n\n        # First conv block\n        layers.append(nn.Conv2d(in_channels, filters[0], kernel_size=filter_size, padding='same'))\n        if use_batchnorm:\n            layers.append(nn.BatchNorm2d(filters[0]))\n        layers.append(activation_fn())\n        if dropout_rate > 0:\n            layers.append(nn.Dropout2d(dropout_rate))\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n\n        # Remaining conv blocks\n        for i in range(1, 5):\n            layers.append(nn.Conv2d(filters[i-1], filters[i], kernel_size=filter_size, padding='same'))\n            if use_batchnorm:\n                layers.append(nn.BatchNorm2d(filters[i]))\n            layers.append(activation_fn())\n            if dropout_rate > 0:\n                layers.append(nn.Dropout2d(dropout_rate))\n            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.features = nn.Sequential(*layers)\n\n        # Calculate output dimensions dynamically\n        with torch.no_grad():\n            x = torch.randn(1, in_channels, *input_size)\n            x = self.features(x)\n            self.flattened_size = x.numel() // x.size(0)\n\n        # Classifier\n        classifier_layers = [\n            nn.Flatten(),\n            nn.Linear(self.flattened_size, dense_neurons),\n            activation_fn()\n        ]\n\n        if dropout_rate > 0:\n            classifier_layers.append(nn.Dropout(dropout_rate))\n\n        classifier_layers.append(nn.Linear(dense_neurons, num_classes))\n\n        self.classifier = nn.Sequential(*classifier_layers)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n# Dictionary of activation functions\nactivation_functions = {\n    'relu': nn.ReLU,\n    'gelu': nn.GELU,\n    'silu': nn.SiLU,\n    'mish': nn.Mish\n}\n\n# Function to train model with wandb logging\ndef train_with_wandb(config=None):\n    with wandb.init(config=config):\n        config = wandb.config\n\n        # Create data loaders\n        train_loader, val_loader, test_loader, num_classes = create_data_loaders(\n            train_dir=config.train_dir,\n            test_dir=config.test_dir,\n            input_size=(config.input_size,config.input_size),\n            batch_size=config.batch_size,\n            data_augmentation=config.data_augmentation\n        )\n\n        # Create model\n        model = FlexibleCNN(\n            num_filters=config.num_filters,\n            filter_size=config.filter_size,\n            activation_fn=activation_functions[config.activation],\n            filter_organization=config.filter_organization,\n            use_batchnorm=config.use_batchnorm,\n            dropout_rate=config.dropout_rate,\n            dense_neurons=config.dense_neurons,\n            num_classes=num_classes,\n            input_size=(config.input_size,config.input_size)\n        )\n\n        # Move model to device\n        model = model.to(device)\n\n        # Print model info\n        print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n        # Define loss and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n\n        # Training loop\n        for epoch in range(config.epochs):\n            # Training phase\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                gc.collect()\n            model.train()\n            train_loss = 0.0\n            correct = 0\n            total = 0\n\n            with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.epochs} [Train]\") as pbar:\n                for inputs, targets in pbar:\n                    inputs, targets = inputs.to(device), targets.to(device)\n\n                    # Forward pass\n                    optimizer.zero_grad()\n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets)\n\n                    # Backward pass\n                    loss.backward()\n                    optimizer.step()\n\n                    # Track metrics\n                    train_loss += loss.item()\n                    _, predicted = outputs.max(1)\n                    total += targets.size(0)\n                    correct += predicted.eq(targets).sum().item()\n\n                    # Update progress bar\n                    pbar.set_postfix({\n                        'loss': train_loss / (pbar.n + 1),\n                        'acc': 100. * correct / total\n                    })\n                    del inputs, targets, outputs, loss\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n\n            train_loss = train_loss / len(train_loader)\n            train_accuracy = 100. * correct / total\n\n            # Validation phase\n            model.eval()\n            val_loss = 0.0\n            correct = 0\n            total = 0\n\n            with torch.no_grad():\n                with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config.epochs} [Val]\") as pbar:\n                    for inputs, targets in pbar:\n                        inputs, targets = inputs.to(device), targets.to(device)\n\n                        # Forward pass\n                        outputs = model(inputs)\n                        loss = criterion(outputs, targets)\n\n                        # Track metrics\n                        val_loss += loss.item()\n                        _, predicted = outputs.max(1)\n                        total += targets.size(0)\n                        correct += predicted.eq(targets).sum().item()\n\n                        # Update progress bar\n                        pbar.set_postfix({\n                            'loss': val_loss / (pbar.n + 1),\n                            'acc': 100. * correct / total\n                        })\n                        # Free memory\n                        del inputs, targets, outputs, loss\n                        if torch.cuda.is_available():\n                            torch.cuda.empty_cache()\n\n            val_loss = val_loss / len(val_loader)\n            val_accuracy = 100. * correct / total\n\n            # Test phase\n            model.eval()\n            test_loss = 0.0\n            correct = 0\n            total = 0\n\n            with torch.no_grad():\n                with tqdm(test_loader, desc=f\"Epoch {epoch+1}/{config.epochs} [Test]\") as pbar:\n                    for inputs, targets in pbar:\n                        inputs, targets = inputs.to(device), targets.to(device)\n\n                        # Forward pass\n                        outputs = model(inputs)\n                        loss = criterion(outputs, targets)\n\n                        # Track metrics\n                        test_loss += loss.item()\n                        _, predicted = outputs.max(1)\n                        total += targets.size(0)\n                        correct += predicted.eq(targets).sum().item()\n\n                        # Update progress bar\n                        pbar.set_postfix({\n                            'loss': test_loss / (pbar.n + 1),\n                            'acc': 100. * correct / total\n                        })\n                        # Free memory\n                        del inputs, targets, outputs, loss\n                        if torch.cuda.is_available():\n                            torch.cuda.empty_cache()\n\n            test_loss = test_loss / len(test_loader)\n            test_accuracy = 100. * correct / total\n\n            # Log metrics to wandb\n            wandb.log({\n                'epoch': epoch + 1,\n                'train_loss': train_loss,\n                'train_accuracy': train_accuracy,\n                'val_loss': val_loss,\n                'val_accuracy': val_accuracy,\n                'test_loss':test_loss,\n                'test_accuracy':test_accuracy\n            })\n            create_prediction_grid(model, test_loader, CLASSES)\n\n            print(f\"Epoch {epoch+1}/{config.epochs} - \"\n                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\"\n                  f\"Test Loss: {test_loss:.4f}, test Acc: {test_accuracy:.2f}%\")\n\n        return model, val_accuracy\n\n# Define sweep configuration\nsweep_config = {\n    'method': 'grid',  # Bayesian optimization for efficient search\n    'metric': {\n        'name': 'val_accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'train_dir': {\n            'value': '/kaggle/working/nature_12k/inaturalist_12K/train'\n        },\n        'test_dir': {\n            'value': '/kaggle/working/nature_12k/inaturalist_12K/val'\n        },\n        'batch_size': {\n            'values': [32]  # Small batch sizes to avoid OOM\n        },\n        'input_size':{\n            'values':[600]\n        },\n        'num_filters': {\n            'values': [32]  # Filter counts\n        },\n        'filter_size': {\n            'values': [5]  # Filter sizes\n        },\n        'activation': {\n            'values': ['gelu']  # Activation functions\n        },\n        'filter_organization': {\n            'values': ['double']  # Filter organization strategies\n        },\n        'data_augmentation': {\n            'values': [True]  # Whether to use data augmentation\n        },\n        'use_batchnorm': {\n            'values': [False]  # Whether to use batch normalization\n        },\n        'dropout_rate': {\n            'values': [0.2,0.0]  # Dropout rates\n        },\n        'dense_neurons': {\n            'values': [128]  # Number of neurons in dense layer\n        },\n        'learning_rate': {\n            'values': [0.0001]  # Learning rates\n        },\n        'epochs': {\n            'value': 10  # Fixed number of epochs for all runs\n        }\n    }\n}\n\n# Initialize wandb\nwandb.login(key=\"2b8654ea1d7143307fd59d1ea1bda5bc9f6fef77\")\n\n# Create the sweep\nsweep_id = wandb.sweep(sweep_config, project=\"da6401_assignment2\")\n\n# Run the sweep (limit to 20 runs for efficiency)\nwandb.agent(sweep_id, entity=\"cs24m048-iit-madras\", project=\"da6401_assignment2\" , function=train_with_wandb, count=2)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:43:16.648314Z","iopub.execute_input":"2025-04-13T13:43:16.649008Z","iopub.status.idle":"2025-04-13T15:16:18.192867Z","shell.execute_reply.started":"2025-04-13T13:43:16.648984Z","shell.execute_reply":"2025-04-13T15:16:18.191690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown --id 15ko_kLn_UQO6tZzrG1IWnNbZzOC-uAdw\n!unzip -q nature_12K.zip -d nature_12k","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T04:15:53.039989Z","iopub.execute_input":"2025-04-17T04:15:53.040269Z","iopub.status.idle":"2025-04-17T04:17:11.124131Z","shell.execute_reply.started":"2025-04-17T04:15:53.040246Z","shell.execute_reply":"2025-04-17T04:17:11.123108Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=15ko_kLn_UQO6tZzrG1IWnNbZzOC-uAdw\nFrom (redirected): https://drive.google.com/uc?id=15ko_kLn_UQO6tZzrG1IWnNbZzOC-uAdw&confirm=t&uuid=fc3bcaf7-a259-4889-8502-b00416eee889\nTo: /kaggle/working/nature_12K.zip\n100%|███████████████████████████████████████| 3.82G/3.82G [00:34<00:00, 111MB/s]\nreplace nature_12k/inaturalist_12K/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T04:29:19.916755Z","iopub.execute_input":"2025-04-17T04:29:19.917072Z","iopub.status.idle":"2025-04-17T04:29:20.103949Z","shell.execute_reply.started":"2025-04-17T04:29:19.917048Z","shell.execute_reply":"2025-04-17T04:29:20.103218Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nfrom torch.cuda import amp\nfrom tqdm import tqdm\nimport numpy as np\nimport math\n\n# Free up GPU memory\ntorch.cuda.empty_cache()\n\n# Set PyTorch memory allocation config to avoid fragmentation\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n\n# Configuration\nBATCH_SIZE = 16  # Reduced batch size to avoid OOM\nEPOCHS = 20\nLEARNING_RATE = 0.0001\nIMAGE_SIZE = 400  # Reduced image size to save memory\nNUM_WORKERS = 4\nACCUMULATION_STEPS = 4  # Gradient accumulation steps\nPATIENCE = 5  # Early stopping patience\nDATASET_PATH = \"/kaggle/working/nature_12k/inaturalist_12K\"  # Update with your dataset path\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB\")\n    print(f\"Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.1f} MB\")\n\n# Custom transform to handle image orientation\nclass RotateIfNeeded:\n    def __call__(self, img):\n        width, height = img.size\n        if height > width:\n            return img.transpose(2)  # Rotate 90 degrees if portrait\n        return img\n\ndef center_crop_image(img, target_size=(600, 600)):\n    \"\"\"Crop the center of the image to the target size.\"\"\"\n    width, height = img.size\n    target_width, target_height = target_size\n    left = max(0, (width - target_width) // 2)\n    top = max(0, (height - target_height) // 2)\n    right = left + target_width\n    bottom = top + target_height\n    return img.crop((left, top, right, bottom))\n# Define data transforms\ntrain_transform = transforms.Compose([\n    RotateIfNeeded(),\n    transforms.Lambda(lambda img: center_crop_image(img, target_size=(IMAGE_SIZE, IMAGE_SIZE))),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    RotateIfNeeded(),\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load datasets\nprint(f\"Loading datasets from {DATASET_PATH}\")\ntrain_dataset = datasets.ImageFolder(DATASET_PATH + \"/train\", transform=train_transform)\nval_dataset = datasets.ImageFolder(DATASET_PATH + \"/val\", transform=val_transform)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\n# Get number of classes\nnum_classes = len(train_dataset.classes)\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")\nprint(f\"Class names: {train_dataset.classes}\")\n\n# Load EfficientNet V2 Small model with pre-trained weights (smaller than Large to save memory)\nmodel = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n\n# Replace classifier with one matching iNaturalist classes\nin_features = model.classifier[1].in_features\nmodel.classifier[1] = nn.Linear(in_features, num_classes)\n\n# Move model to GPU\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Initialize mixed precision scaler\nscaler = amp.GradScaler()\n\n# Create directory for saving models\nos.makedirs(\"models\", exist_ok=True)\n\n# Training loop\nbest_val_accuracy = 0.0\nearly_stopping_counter = 0\n\nfor epoch in range(EPOCHS):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    optimizer.zero_grad()\n    \n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Training\")\n    \n    for i, (inputs, labels) in enumerate(progress_bar):\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Use mixed precision\n        with amp.autocast():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels) / ACCUMULATION_STEPS\n        \n        # Scale gradients and backpropagate\n        scaler.scale(loss).backward()\n        \n        # Update weights after accumulating gradients\n        if (i + 1) % ACCUMULATION_STEPS == 0 or (i + 1) == len(train_loader):\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        # Track statistics (multiply by accumulation steps to get true loss)\n        train_loss += loss.item() * inputs.size(0) * ACCUMULATION_STEPS\n        _, predicted = torch.max(outputs, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n        \n        # Update progress bar\n        progress_bar.set_postfix({\n            \"loss\": f\"{loss.item() * ACCUMULATION_STEPS:.4f}\",\n            \"accuracy\": f\"{100.0 * train_correct / train_total:.2f}%\"\n        })\n    \n    train_loss = train_loss / len(train_dataset)\n    train_accuracy = 100.0 * train_correct / train_total\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Validation\")\n        for inputs, labels in progress_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # For validation, we don't need mixed precision\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            # Track statistics\n            val_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            \n            # Update progress bar\n            progress_bar.set_postfix({\n                \"loss\": f\"{loss.item():.4f}\",\n                \"accuracy\": f\"{100.0 * val_correct / val_total:.2f}%\"\n            })\n    \n    val_loss = val_loss / len(val_dataset)\n    val_accuracy = 100.0 * val_correct / val_total\n    \n    # Print epoch summary\n    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n    \n    # Save the best model\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        torch.save(model.state_dict(), os.path.join(\"models\", \"best_model.pth\"))\n        print(f\"New best model saved with validation accuracy: {best_val_accuracy:.2f}%\")\n        early_stopping_counter = 0\n    else:\n        early_stopping_counter += 1\n        print(f\"EarlyStopping: {early_stopping_counter}/{PATIENCE}\")\n        \n        # Implement early stopping\n        if early_stopping_counter >= PATIENCE:\n            print(\"Early stopping triggered\")\n            break\n    \n    # Save checkpoint\n    checkpoint_path = os.path.join(\"models\", f\"checkpoint_epoch_{epoch+1}.pth\")\n    torch.save({\n        \"epoch\": epoch,\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss,\n        \"val_accuracy\": val_accuracy\n    }, checkpoint_path)\n    print(f\"Checkpoint saved to {checkpoint_path}\")\n\n# Load the best model for final evaluation\nprint(\"Loading best model for final evaluation...\")\nmodel.load_state_dict(torch.load(os.path.join(\"models\", \"best_model.pth\")))\nmodel.eval()\n\n# Perform final evaluation\nval_correct = 0\nval_total = 0\n\nprint(\"Final Evaluation\")\nprogress_bar = tqdm(val_loader, desc=\"Final evaluation\")\n\nwith torch.no_grad():\n    for inputs, labels in progress_bar:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        val_total += labels.size(0)\n        val_correct += (predicted == labels).sum().item()\n        \n        # Update progress bar\n        progress_bar.set_postfix({\"accuracy\": f\"{100.0 * val_correct / val_total:.2f}%\"})\n\nfinal_accuracy = 100.0 * val_correct / val_total\nprint(f\"Final validation accuracy: {final_accuracy:.2f}%\")\nprint(f\"Best validation accuracy: {best_val_accuracy:.2f}%\")\n\n# Save class mapping\nclass_mapping = {i: cls for i, cls in enumerate(train_dataset.classes)}\nprint(f\"Class mapping: {class_mapping}\")\n\nprint(\"Training completed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:50:55.091726Z","iopub.execute_input":"2025-04-16T18:50:55.092040Z","iopub.status.idle":"2025-04-16T19:00:40.110786Z","shell.execute_reply.started":"2025-04-16T18:50:55.092014Z","shell.execute_reply":"2025-04-16T19:00:40.109753Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla T4\nMemory allocated: 351.7 MB\nMemory cached: 742.0 MB\nLoading datasets from /kaggle/working/nature_12k/inaturalist_12K\nNumber of classes: 10\nNumber of training samples: 9999\nNumber of validation samples: 2000\nClass names: ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2245642087.py:104: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = amp.GradScaler()\nEpoch 1/20 Training:   0%|          | 0/625 [00:00<?, ?it/s]/tmp/ipykernel_31/2245642087.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nEpoch 1/20 Training: 100%|██████████| 625/625 [02:19<00:00,  4.48it/s, loss=0.5315, accuracy=75.03%]\nEpoch 1/20 Validation: 100%|██████████| 125/125 [00:17<00:00,  7.25it/s, loss=0.1300, accuracy=89.10%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Train Loss: 0.8898, Train Acc: 75.03%, Val Loss: 0.3625, Val Acc: 89.10%\nNew best model saved with validation accuracy: 89.10%\nCheckpoint saved to models/checkpoint_epoch_1.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20 Training: 100%|██████████| 625/625 [02:17<00:00,  4.53it/s, loss=0.1385, accuracy=89.43%]\nEpoch 2/20 Validation: 100%|██████████| 125/125 [00:17<00:00,  7.33it/s, loss=0.3115, accuracy=89.90%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Train Loss: 0.3486, Train Acc: 89.43%, Val Loss: 0.3521, Val Acc: 89.90%\nNew best model saved with validation accuracy: 89.90%\nCheckpoint saved to models/checkpoint_epoch_2.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20 Training: 100%|██████████| 625/625 [02:18<00:00,  4.53it/s, loss=0.3926, accuracy=92.58%]\nEpoch 3/20 Validation: 100%|██████████| 125/125 [00:17<00:00,  7.30it/s, loss=0.2655, accuracy=89.20%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Train Loss: 0.2434, Train Acc: 92.58%, Val Loss: 0.3413, Val Acc: 89.20%\nEarlyStopping: 1/5\nCheckpoint saved to models/checkpoint_epoch_3.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20 Training:  83%|████████▎ | 520/625 [01:55<00:23,  4.50it/s, loss=0.5478, accuracy=94.98%]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2245642087.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Track statistics (multiply by accumulation steps to get true loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mACCUMULATION_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mtrain_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":39},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nfrom torch.cuda import amp\nfrom tqdm import tqdm\nimport numpy as np\nimport wandb\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Free up GPU memory\ntorch.cuda.empty_cache()\n\n# Set PyTorch memory allocation config\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB\")\n    print(f\"Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.1f} MB\")\n\n# Custom transform to handle image orientation\nclass RotateIfNeeded:\n    def __call__(self, img):\n        width, height = img.size\n        if height > width:\n            return img.transpose(2)  # Rotate 90 degrees if portrait\n        return img\n\ndef center_crop_image(img, target_size=(400, 400)):\n    \"\"\"Crop the center of the image to the target size.\"\"\"\n    width, height = img.size\n    target_width, target_height = target_size\n    left = max(0, (width - target_width) // 2)\n    top = max(0, (height - target_height) // 2)\n    right = left + target_width\n    bottom = top + target_height\n    return img.crop((left, top, right, bottom))\n\ndef count_parameters(model):\n    \"\"\"Count the number of trainable and non-trainable parameters\"\"\"\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    non_trainable = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n    return trainable, non_trainable\n\ndef freeze_all_layers_except_last(model):\n    \"\"\"Freeze all layers except the last fully connected layer\"\"\"\n    # Freeze all parameters\n    for param in model.parameters():\n        param.requires_grad = False\n    \n    # Unfreeze the last layer (classifier)\n    for param in model.classifier[1].parameters():\n        param.requires_grad = True\n    \n    return model\n\ndef freeze_first_k_layers(model, k):\n    \"\"\"Freeze first K layers, train the rest\"\"\"\n    # First, unfreeze all parameters\n    for param in model.parameters():\n        param.requires_grad = True\n    \n    # Get list of all layers in features\n    features = list(model.features)\n    \n    # Freeze the first k layers\n    for i in range(min(k, len(features))):\n        for param in features[i].parameters():\n            param.requires_grad = False\n    \n    return model\n\ndef no_freezing(model):\n    \"\"\"Train all layers (no freezing)\"\"\"\n    for param in model.parameters():\n        param.requires_grad = True\n    return model\n\n# Function that will be called by wandb sweep\ndef train_model_sweep(config=None):\n    with wandb.init(config=config):\n        # Access all hyperparameters through wandb.config\n        config = wandb.config\n        \n        # Data transformations\n        train_transform = transforms.Compose([\n            RotateIfNeeded(),\n            transforms.Lambda(lambda img: center_crop_image(img, target_size=(config.image_size, config.image_size))),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(15),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n\n        val_transform = transforms.Compose([\n            RotateIfNeeded(),\n            transforms.Lambda(lambda img: center_crop_image(img, target_size=(config.image_size, config.image_size))),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        \n        test_transform = transforms.Compose([\n            RotateIfNeeded(),\n            transforms.Lambda(lambda img: center_crop_image(img, target_size=(config.image_size, config.image_size))),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        \n        # Load full training dataset\n        print(f\"Loading datasets from {config.dataset_path}\")\n        full_train_dataset = datasets.ImageFolder(config.dataset_path + \"/train\", transform=train_transform)\n        \n        # Get targets for stratified split\n        targets = np.array([label for _, label in full_train_dataset.samples])\n        \n        # Create stratified split - 80% train, 20% validation\n        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n        train_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n        \n        # Verify the class distribution\n        train_labels = targets[train_idx]\n        val_labels = targets[val_idx]\n        unique_labels = np.unique(targets)\n        \n        print(\"\\nClass distribution:\")\n        for label in unique_labels:\n            train_count = np.sum(train_labels == label)\n            val_count = np.sum(val_labels == label)\n            total_count = np.sum(targets == label)\n            print(f\"Class {full_train_dataset.classes[label]}: \"\n                  f\"Train {train_count}/{total_count} ({train_count/total_count:.2f}), \"\n                  f\"Val {val_count}/{total_count} ({val_count/total_count:.2f})\")\n        \n        # Create Subset objects for train and validation\n        train_subset = Subset(full_train_dataset, train_idx)\n        val_subset = Subset(full_train_dataset, val_idx)\n        \n        # Load the test dataset (using the val folder)\n        test_dataset = datasets.ImageFolder(config.dataset_path + \"/val\", transform=test_transform)\n        \n        # Create data loaders\n        train_loader = DataLoader(\n            train_subset, \n            batch_size=config.batch_size, \n            shuffle=True, \n            num_workers=config.num_workers, \n            pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            val_subset, \n            batch_size=config.batch_size, \n            shuffle=False, \n            num_workers=config.num_workers, \n            pin_memory=True\n        )\n        \n        test_loader = DataLoader(\n            test_dataset, \n            batch_size=config.batch_size, \n            shuffle=False, \n            num_workers=config.num_workers, \n            pin_memory=True\n        )\n        \n        # Get number of classes\n        num_classes = len(full_train_dataset.classes)\n        print(f\"\\nNumber of classes: {num_classes}\")\n        print(f\"Full training dataset: {len(full_train_dataset)} images\")\n        print(f\"Training subset: {len(train_subset)} images ({len(train_subset)/len(full_train_dataset):.1%})\")\n        print(f\"Validation subset: {len(val_subset)} images ({len(val_subset)/len(full_train_dataset):.1%})\")\n        print(f\"Test dataset: {len(test_dataset)} images\")\n\n        # Load model\n        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n        \n        # Replace classifier\n        in_features = model.classifier[1].in_features\n        model.classifier[1] = nn.Linear(in_features, num_classes)\n        \n        # Apply freezing strategy based on config\n        if config.freezing_strategy == \"freeze_all_except_last\":\n            model = freeze_all_layers_except_last(model)\n        elif config.freezing_strategy == \"freeze_first_k_layers\":\n            model = freeze_first_k_layers(model, config.freeze_k_layers)\n        elif config.freezing_strategy == \"no_freezing\":\n            model = no_freezing(model)\n        else:\n            raise ValueError(f\"Unknown freezing strategy: {config.freezing_strategy}\")\n        \n        # Count parameters\n        trainable_params, non_trainable_params = count_parameters(model)\n        total_params = trainable_params + non_trainable_params\n        percent_trainable = 100 * trainable_params / total_params\n        \n        print(f\"\\nTrainable parameters: {trainable_params:,} ({percent_trainable:.2f}%)\")\n        print(f\"Non-trainable parameters: {non_trainable_params:,} ({100-percent_trainable:.2f}%)\")\n        print(f\"Total parameters: {total_params:,}\")\n        \n        # Log parameter counts to wandb\n        wandb.log({\n            \"trainable_params\": trainable_params,\n            \"non_trainable_params\": non_trainable_params,\n            \"percent_trainable\": percent_trainable\n        })\n        \n        # Move model to device\n        model = model.to(device)\n        \n        # Define loss function and optimizer (only train parameters that require gradients)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(\n            filter(lambda p: p.requires_grad, model.parameters()), \n            lr=config.learning_rate\n        )\n        \n        # Initialize mixed precision scaler\n        scaler = amp.GradScaler()\n        \n        # Create directory for saving models\n        os.makedirs(f\"models/{wandb.run.id}\", exist_ok=True)\n        \n        # Training loop - fixed at 10 epochs, no early stopping\n        best_val_accuracy = 0.0\n        \n        for epoch in range(config.epochs):\n            # Training phase\n            model.train()\n            train_loss = 0.0\n            train_correct = 0\n            train_total = 0\n            optimizer.zero_grad()\n            \n            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.epochs} Training\")\n            \n            for i, (inputs, labels) in enumerate(progress_bar):\n                inputs, labels = inputs.to(device), labels.to(device)\n                \n                # Use mixed precision\n                with amp.autocast():\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels) / config.accumulation_steps\n                \n                # Scale gradients and backpropagate\n                scaler.scale(loss).backward()\n                \n                # Update weights after accumulating gradients\n                if (i + 1) % config.accumulation_steps == 0 or (i + 1) == len(train_loader):\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                \n                # Track statistics\n                train_loss += loss.item() * inputs.size(0) * config.accumulation_steps\n                _, predicted = torch.max(outputs, 1)\n                train_total += labels.size(0)\n                train_correct += (predicted == labels).sum().item()\n                \n                # Update progress bar\n                progress_bar.set_postfix({\n                    \"loss\": f\"{loss.item() * config.accumulation_steps:.4f}\",\n                    \"accuracy\": f\"{100.0 * train_correct / train_total:.2f}%\"\n                })\n            \n            train_loss = train_loss / len(train_subset)\n            train_accuracy = 100.0 * train_correct / train_total\n            \n            # Validation phase\n            model.eval()\n            val_loss = 0.0\n            val_correct = 0\n            val_total = 0\n            \n            with torch.no_grad():\n                progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config.epochs} Validation\")\n                for inputs, labels in progress_bar:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    \n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    \n                    # Track statistics\n                    val_loss += loss.item() * inputs.size(0)\n                    _, predicted = torch.max(outputs, 1)\n                    val_total += labels.size(0)\n                    val_correct += (predicted == labels).sum().item()\n                    \n                    # Update progress bar\n                    progress_bar.set_postfix({\n                        \"loss\": f\"{loss.item():.4f}\",\n                        \"accuracy\": f\"{100.0 * val_correct / val_total:.2f}%\"\n                    })\n            \n            val_loss = val_loss / len(val_subset)\n            val_accuracy = 100.0 * val_correct / val_total\n            \n            # Print epoch summary\n            print(f\"Epoch {epoch+1}/{config.epochs} - \"\n                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n            \n            # Log to wandb\n            wandb.log({\n                \"epoch\": epoch + 1,\n                \"train_loss\": train_loss,\n                \"train_accuracy\": train_accuracy,\n                \"val_loss\": val_loss,\n                \"val_accuracy\": val_accuracy\n            })\n            \n            # Save the best model\n            if val_accuracy > best_val_accuracy:\n                best_val_accuracy = val_accuracy\n                torch.save(model.state_dict(), os.path.join(f\"models/{wandb.run.id}\", \"best_model.pth\"))\n                print(f\"New best model saved with validation accuracy: {best_val_accuracy:.2f}%\")\n        \n        # Load the best model for final evaluation on test set\n        print(\"\\nLoading best model for final test evaluation...\")\n        model.load_state_dict(torch.load(os.path.join(f\"models/{wandb.run.id}\", \"best_model.pth\")))\n        model.eval()\n        \n        # Perform test evaluation\n        test_loss = 0.0\n        test_correct = 0\n        test_total = 0\n        \n        print(\"Test Evaluation\")\n        progress_bar = tqdm(test_loader, desc=\"Testing on val folder (test set)\")\n        \n        with torch.no_grad():\n            for inputs, labels in progress_bar:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                test_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs, 1)\n                test_total += labels.size(0)\n                test_correct += (predicted == labels).sum().item()\n                \n                # Update progress bar\n                progress_bar.set_postfix({\n                    \"loss\": f\"{loss.item():.4f}\",\n                    \"accuracy\": f\"{100.0 * test_correct / test_total:.2f}%\"\n                })\n        \n        test_loss = test_loss / len(test_dataset)\n        test_accuracy = 100.0 * test_correct / test_total\n        \n        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n        print(f\"Best validation accuracy: {best_val_accuracy:.2f}%\")\n        \n        # Log final metrics to wandb\n        wandb.log({\n            \"test_loss\": test_loss,\n            \"test_accuracy\": test_accuracy,\n            \"best_val_accuracy\": best_val_accuracy\n        })\n\n# Define sweep configuration\nsweep_config = {\n    'method': 'grid',  # Try all combinations since we have a focused set of strategies\n    'metric': {\n        'name': 'test_accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'freezing_strategy': {\n            'values': ['freeze_all_except_last', 'freeze_first_k_layers', 'no_freezing']\n        },\n        'freeze_k_layers': {\n            'value': 3  # Only used when freezing_strategy is 'freeze_first_k_layers'\n        },\n        'learning_rate': {\n            'value': 0.0001\n        },\n        'batch_size': {\n            'value': 16\n        },\n        'epochs': {\n            'value': 10  # Fixed at 10 epochs as requested\n        },\n        'image_size': {\n            'value': 400  # Sweet spot as per your findings\n        },\n        'num_workers': {\n            'value': 4\n        },\n        'accumulation_steps': {\n            'value': 4\n        },\n        'dataset_path': {\n            'value': '/kaggle/working/nature_12k/inaturalist_12K'  # Update with your dataset path\n        }\n    }\n}\n\nwandb.login(key=\"2b8654ea1d7143307fd59d1ea1bda5bc9f6fef77\")\n# Initialize wandb sweep\nsweep_id = wandb.sweep(sweep_config, project=\"da6401_assignment2\")\n\n# Run the sweep agent\nwandb.agent(sweep_id, train_model_sweep)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:21:38.364648Z","iopub.execute_input":"2025-04-17T05:21:38.364937Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nGPU: Tesla T4\nMemory allocated: 16.2 MB\nMemory cached: 50.0 MB\nCreate sweep with ID: mgg0mjal\nSweep URL: https://wandb.ai/cs24m048-iit-madras/da6401_assignment2/sweeps/mgg0mjal\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s7ic27w0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \taccumulation_steps: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset_path: /kaggle/working/nature_12k/inaturalist_12K\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_k_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfreezing_strategy: freeze_all_except_last\n\u001b[34m\u001b[1mwandb\u001b[0m: \timage_size: 400\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_052146-s7ic27w0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m048-iit-madras/da6401_assignment2/runs/s7ic27w0' target=\"_blank\">lunar-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m048-iit-madras/da6401_assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m048-iit-madras/da6401_assignment2/sweeps/mgg0mjal' target=\"_blank\">https://wandb.ai/cs24m048-iit-madras/da6401_assignment2/sweeps/mgg0mjal</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m048-iit-madras/da6401_assignment2' target=\"_blank\">https://wandb.ai/cs24m048-iit-madras/da6401_assignment2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m048-iit-madras/da6401_assignment2/sweeps/mgg0mjal' target=\"_blank\">https://wandb.ai/cs24m048-iit-madras/da6401_assignment2/sweeps/mgg0mjal</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m048-iit-madras/da6401_assignment2/runs/s7ic27w0' target=\"_blank\">https://wandb.ai/cs24m048-iit-madras/da6401_assignment2/runs/s7ic27w0</a>"},"metadata":{}},{"name":"stdout","text":"Loading datasets from /kaggle/working/nature_12k/inaturalist_12K\n\nClass distribution:\nClass Amphibia: Train 800/1000 (0.80), Val 200/1000 (0.20)\nClass Animalia: Train 800/1000 (0.80), Val 200/1000 (0.20)\nClass Arachnida: Train 800/1000 (0.80), Val 200/1000 (0.20)\nClass Aves: Train 800/1000 (0.80), Val 200/1000 (0.20)\nClass Fungi: Train 799/999 (0.80), Val 200/999 (0.20)\nClass Insecta: Train 800/1000 (0.80), Val 200/1000 (0.20)\nClass Mammalia: Train 800/1000 (0.80), Val 200/1000 (0.20)\nClass Mollusca: Train 800/1000 (0.80), Val 200/1000 (0.20)\nClass Plantae: Train 800/1000 (0.80), Val 200/1000 (0.20)\nClass Reptilia: Train 800/1000 (0.80), Val 200/1000 (0.20)\n\nNumber of classes: 10\nFull training dataset: 9999 images\nTraining subset: 7999 images (80.0%)\nValidation subset: 2000 images (20.0%)\nTest dataset: 2000 images\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3009143861.py:225: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = amp.GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"\nTrainable parameters: 12,810 (0.06%)\nNon-trainable parameters: 20,177,488 (99.94%)\nTotal parameters: 20,190,298\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 Training:   0%|          | 0/500 [00:00<?, ?it/s]/tmp/ipykernel_31/3009143861.py:247: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nEpoch 1/10 Training: 100%|██████████| 500/500 [00:57<00:00,  8.65it/s, loss=1.8327, accuracy=44.63%]\nEpoch 1/10 Validation: 100%|██████████| 125/125 [00:23<00:00,  5.39it/s, loss=1.8260, accuracy=72.60%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 - Train Loss: 2.0474, Train Acc: 44.63%, Val Loss: 1.8371, Val Acc: 72.60%\nNew best model saved with validation accuracy: 72.60%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 Training: 100%|██████████| 500/500 [00:56<00:00,  8.87it/s, loss=1.4532, accuracy=69.97%]\nEpoch 2/10 Validation: 100%|██████████| 125/125 [00:23<00:00,  5.33it/s, loss=1.5742, accuracy=80.60%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 - Train Loss: 1.6387, Train Acc: 69.97%, Val Loss: 1.5044, Val Acc: 80.60%\nNew best model saved with validation accuracy: 80.60%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 Training: 100%|██████████| 500/500 [00:56<00:00,  8.91it/s, loss=1.2314, accuracy=74.78%]\nEpoch 3/10 Validation: 100%|██████████| 125/125 [00:23<00:00,  5.30it/s, loss=1.3288, accuracy=81.95%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 - Train Loss: 1.3547, Train Acc: 74.78%, Val Loss: 1.2514, Val Acc: 81.95%\nNew best model saved with validation accuracy: 81.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10 Training: 100%|██████████| 500/500 [00:55<00:00,  9.00it/s, loss=0.7072, accuracy=76.30%]\nEpoch 4/10 Validation:   3%|▎         | 4/125 [00:01<00:30,  4.01it/s, loss=1.1168, accuracy=81.25%]","output_type":"stream"}],"execution_count":null}]}